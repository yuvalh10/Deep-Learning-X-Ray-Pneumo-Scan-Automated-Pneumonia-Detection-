{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_fSb51KWkDq2bVWAKI2IIcQ8fwtXRiDw","timestamp":1712069407959},{"file_id":"1wCaZ3yu0zDN_4g-CoL4GJtTRH8pw7cuw","timestamp":1712039075849}],"gpuType":"A100","collapsed_sections":["LVNZ95Wvm83D","-eQ1RPSZmCFN"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Import Libraries and Modules"],"metadata":{"id":"ELxKPiu_myyD"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import os\n","import random\n","import re\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","from zipfile import ZipFile\n","from glob import glob\n","from google.colab import drive\n","from keras.models import Sequential\n","from PIL import Image\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import datasets, layers, optimizers\n","from tensorflow.keras.layers import Dropout,GlobalAveragePooling2D, Dense\n","from matplotlib import pyplot as plt\n","from tensorflow.keras.optimizers import Adagrad\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import precision_score, recall_score,f1_score\n","from sklearn.metrics import auc\n","from tensorflow.keras.applications import MobileNetV2"],"metadata":{"id":"MmkQAqlLm0MI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extract the images and clasiffing into 3 categories\n","# (healthy, virus, bacterial)"],"metadata":{"id":"LVNZ95Wvm83D"}},{"cell_type":"code","source":["drive.mount('/content/drive')\n","\n","file_name = '/content/drive/My Drive/Final Project DL & ML/chest-xray.zip'\n","\n","with ZipFile(file_name, 'r') as zip_file:\n","  zip_file.extractall()  # Extract all contents of the ZIP file.\n","  print('Done')\n","\n","train_path = '/content/chest_xray/chest_xray/train'\n","val_path = '/content/chest_xray/chest_xray/val'\n","test_path = '/content/chest_xray/chest_xray/test'\n","\n","pre_path = '/content/chest_xray/chest_xray/'\n","\n","# Define paths for different sets and classes.\n","train_normal_dir = pre_path + 'train/NORMAL/'\n","train_pneu_dir = pre_path + 'train/PNEUMONIA/'\n","\n","test_normal_dir = pre_path + 'test/NORMAL/'\n","test_pneu_dir = pre_path + 'test/PNEUMONIA/'\n","\n","val_normal_dir = pre_path + 'val/NORMAL/'\n","val_pneu_dir = pre_path + 'val/PNEUMONIA/'\n","\n","\n","#initiazing the healthy, bacteria and virus cases of the images\n","virus = [] #1493 images total\n","bacteria = [] #2780 images total\n","healthy = [] #1583 images total\n","\n","healthy += glob(train_normal_dir + '*jpeg')\n","\n","#By using Regular Expressions we sorting the cases properly\n","#0- healthy, 1- virus case, 2 - bacteria case\n","for i in os.listdir(train_pneu_dir):\n","  if(re.search(\"virus.*jpeg\", i)):\n","      virus.append([train_pneu_dir+i,1])\n","  elif(re.search(\"bacteria.*jpeg\", i)):\n","      bacteria.append([train_pneu_dir+i,2])\n","\n","healthy += glob(test_normal_dir + '*jpeg')\n","\n","for i in os.listdir(test_pneu_dir):\n","  if(re.search(\"virus.*jpeg\", i)):\n","      virus.append([test_pneu_dir+i,1])\n","  elif(re.search(\"bacteria.*jpeg\", i)):\n","      bacteria.append([test_pneu_dir+i,2])\n","\n","healthy += glob(val_normal_dir + '*jpeg')\n","\n","for i in os.listdir(val_pneu_dir):\n","  if(re.search(\"virus.*jpeg\", i)):\n","      virus.append([val_pneu_dir+i,1])\n","  elif(re.search(\"bacteria.*jpeg\", i)):\n","      bacteria.append([val_pneu_dir+i,2])\n","\n","for i in range(len(healthy)):\n","  healthy[i]=(healthy[i],0)\n","\n","for lst in [healthy,virus,bacteria]:\n","  random.shuffle(lst)"],"metadata":{"id":"5uFuYeotlmK9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712212239939,"user_tz":-180,"elapsed":76904,"user":{"displayName":"יובל המר","userId":"00531576540532881377"}},"outputId":"1fb1ea47-67cc-42b3-bd49-839776e00d3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Done\n"]}]},{"cell_type":"markdown","source":["# Creating the test, train and validation sets"],"metadata":{"id":"-eQ1RPSZmCFN"}},{"cell_type":"code","source":["# 20% validation and test sets, 60% train set from all data\n","# Splitting data into train, validation, and test sets\n","val = virus[:300] + bacteria[:556] + healthy[:316]\n","test = virus[300:600] + bacteria[556:1112] + healthy[316:632]\n","train = virus[600:] + bacteria[1112:] + healthy[632:]\n","\n","def NormalPixels(data):\n","    # Normalize the pixels on every image and label\n","    normal = []\n","    labels = []\n","\n","    for im_path,label in data:\n","        # Assigning labels based on the type of pneumonia\n","        if label == 2:\n","            labels.append(1)  # Label 1 for bacteria pneumonia\n","        else:\n","            labels.append(label)\n","\n","       # Open the image and convert it to RGB\n","        image = Image.open(im_path).convert(\"RGB\")\n","\n","        # Resize the image to 180x180\n","        resized_image = image.resize((180, 180))\n","\n","        # Convert the resized image to a NumPy array\n","        image_array = np.array(resized_image)\n","\n","        # Normalize the pixel values\n","        im_normal = image_array / 255.0\n","\n","        normal.append(im_normal)\n","\n","    return np.array(normal), np.array(labels)\n","\n","def CreatingtVal(val):\n","    val_norm, val_label = NormalPixels(val)  # Normalize validation data\n","    return val_norm, val_label\n","\n","def CreatingtTrain(train):\n","    train_norm, train_label = NormalPixels(train)  # Normalize training data\n","    return train_norm, train_label\n","\n","def CreatingtTest(test):\n","    test_norm, test_label = NormalPixels(test)  # Normalize test data\n","    return test_norm, test_label\n","\n","val_norm, val_label = CreatingtVal(val)\n","train_norm, train_label = CreatingtTrain(train)\n","test_norm, test_label = CreatingtTest(test)"],"metadata":{"id":"qzSaZAjpmBMA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Building the Transfer Learning model"],"metadata":{"id":"1nNPX65ynZ27"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DhD1m9fed6Yp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5c7ca87-30ec-4407-c9e1-6cf0cebf86fd"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]},{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","9406464/9406464 [==============================] - 0s 0us/step\n","Number of layers in the base model:  154\n"]}],"source":["base_model = tf.keras.applications.MobileNetV2(input_shape=(180, 180, 3), include_top=False, weights='imagenet', input_tensor=None, pooling=None, classes=None)\n","base_model.trainable = True\n","print(\"Number of layers in the base model: \", len(base_model.layers))\n","# Fine-tune from this layer onwards\n","fine_tune = 60\n","\n","# Freeze all layers before the fine-tuning layer\n","for layer in base_model.layers[:fine_tune]:\n","    layer.trainable = False\n","\n","preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n","global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n","\n","inputs = tf.keras.Input(shape=(180, 180, 3))\n","x = preprocess_input(inputs)\n","x = base_model(x, training=False)\n","\n","# Additional layers\n","x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n","x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n","x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n","x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n","\n","x = global_average_layer(x)\n","x = tf.keras.layers.Dropout(0.4)(x)\n","outputs = prediction_layer(x)\n","\n","transfer_learning = tf.keras.Model(inputs, outputs)\n","\n","base_learning_rate = 0.0001\n","transfer_learning.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=base_learning_rate,weight_decay = 1e-6 ),\n","                           loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n","                           metrics=['accuracy'])\n","\n","history = transfer_learning.fit(train_norm, train_label, epochs=100, batch_size=20,\n","                                validation_data=(val_norm, val_label), verbose=1)\n","\n","transfer_learning.summary()\n","\n","# Mark the epoch where fine-tuning starts (e.g., epoch 10)\n","fine_tune_lim = 10\n","\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy vs. Number of Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.grid()\n","#plt.axvline(x=fine_tune_lim, color='r', linestyle='--', label='Fine-tuning Start')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","plt.clf() # clear figure\n","plt.plot(history.history['loss'], label='Training loss')\n","\n","plt.plot(history.history['val_loss'], label='Validation loss')\n","plt.title('Training and validation loss vs. Number of Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","#plt.axvline(x=fine_tune_lim, color='r', linestyle='--', label='Fine-tuning Start')\n","plt.grid()\n","plt.legend(loc='upper right')\n","plt.show()\n","\n","# Evaluate the model on the test set\n","test_loss, test_acc = transfer_learning.evaluate(test_norm, test_label, verbose=1)\n","print(\"\\nTest accuracy: \", test_acc)\n","print(\"\\nTest loss: \", test_loss)\n","\n","# Get the predicted probabilities for the test set\n","predicted_probabilities = transfer_learning.predict(test_norm)\n","\n","# Convert probabilities to binary predictions (0 or 1)\n","predicted_labels = (predicted_probabilities > 0.5).astype(int)\n","\n","# Generate the confusion matrix\n","conf_matrix = confusion_matrix(test_label, predicted_labels)\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","# Set labels for the axes\n","plt.xticks(ticks=[0.5, 1.5], labels=['Healthy', 'Sick'])\n","plt.yticks(ticks=[0.5, 1.5], labels=['Healthy', 'Sick'])\n","\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","precision, recall, _ = precision_recall_curve(test_label,predicted_labels)\n","\n","precision_values = []\n","recall_values = []\n","f1_values = []\n","\n","# Compute precision, recall, and threshold values for different thresholds\n","for threshold in np.arange(0.1, 0.95, 0.05):\n","    # Apply threshold to predicted probabilities\n","    predicted_labels = (predicted_probabilities > threshold)\n","    # Compute precision and recall\n","    precision = precision_score(test_label, predicted_labels)\n","    recall = recall_score(test_label, predicted_labels)\n","    f1 = f1_score(test_label, predicted_labels)\n","    precision_values.append(precision)\n","    recall_values.append(recall)\n","    f1_values.append(f1)\n","\n","# Plot the precision-recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall_values, precision_values,recall_values,precision_values,'ro')\n","for i in range(len(precision_values)):\n","  plt.text(recall_values[i],precision_values[i],f' f1={f1_values[i]:.3}',fontsize = 7, rotation = 20)\n","plt.xlabel('precision')\n","plt.ylabel('recall')\n","plt.title('Precision-Recall Curve')\n","plt.grid(True)\n","plt.show()\n","\n","# Get the maximum number in the list\n","max_f1 = max(f1_values)\n","\n","# Get the index of the maximum number\n","max_index = f1_values.index(max_f1)\n","\n","threshold = 0.1+0.05*(len(f1_values)-(max_index+1))\n","\n","print(f\"The best F1 score is {max_f1:.3} with threshold of {threshold:.2}\")\n"]},{"cell_type":"markdown","source":["# plot loss and accuracy for the train and validation set"],"metadata":{"id":"FxbPZqqcnn61"}},{"cell_type":"code","source":["plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Training and Validation Accuracy vs. Number of Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.grid()\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","plt.clf() # clear figure\n","plt.plot(history.history['loss'], label='Training loss')\n","\n","plt.plot(history.history['val_loss'], label='Validation loss')\n","plt.title('Training and validation loss vs. Number of Epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.grid()\n","plt.legend(loc='upper right')\n","plt.show()\n","\n","# Evaluate the model on the test set\n","test_loss, test_acc = transfer_learning.evaluate(test_norm, test_label, verbose=1)\n","print(\"\\nTest accuracy: \", test_acc)\n","print(\"\\nTest loss: \", test_loss)\n","\n","# Get the predicted probabilities for the test set\n","predicted_probabilities = transfer_learning.predict(test_norm)\n","\n","# Convert probabilities to binary predictions (0 or 1)\n","predicted_labels = (predicted_probabilities > 0.5).astype(int)\n","\n","# Generate the confusion matrix\n","conf_matrix = confusion_matrix(test_label, predicted_labels)\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","# Set labels for the axes\n","plt.xticks(ticks=[0.5, 1.5], labels=['Healthy', 'Sick'])\n","plt.yticks(ticks=[0.5, 1.5], labels=['Healthy', 'Sick'])\n","\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","precision, recall, _ = precision_recall_curve(test_label,predicted_labels)\n","\n","precision_values = []\n","recall_values = []\n","f1_values = []\n","\n","# Compute precision, recall, and threshold values for different thresholds\n","for threshold in np.arange(0.1, 0.95, 0.05):\n","    # Apply threshold to predicted probabilities\n","    predicted_labels = (predicted_probabilities > threshold)\n","    # Compute precision and recall\n","    precision = precision_score(test_label, predicted_labels)\n","    recall = recall_score(test_label, predicted_labels)\n","    f1 = f1_score(test_label, predicted_labels)\n","    precision_values.append(precision)\n","    recall_values.append(recall)\n","    f1_values.append(f1)\n","\n","# Plot the precision-recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall_values, precision_values,recall_values,precision_values,'ro')\n","for i in range(len(precision_values)):\n","  plt.text(recall_values[i],precision_values[i],f' f1={f1_values[i]:.3}',fontsize = 7, rotation = 20)\n","plt.xlabel('precision')\n","plt.ylabel('recall')\n","plt.title('Precision-Recall Curve')\n","plt.grid(True)\n","plt.show()\n","\n","# Get the maximum number in the list\n","max_f1 = max(f1_values)\n","\n","# Get the index of the maximum number\n","max_index = f1_values.index(max_f1)\n","\n","threshold = 0.1+0.05*(len(f1_values)-(max_index+1))\n","\n","print(f\"The best F1 score is {max_f1:.3} with threshold of {threshold:.2}\")\n"],"metadata":{"id":"h5pv8kasnlm_"},"execution_count":null,"outputs":[]}]}